# Mongo Replica Set on Kubernetes

nodes
- 1 Kubernetes master
- at least 4 Kubernetes minion nodes

Kubernetes spec deployed
-3 replication controllers
-3 services

Mongo configuration
 - 3 nodes
   - 1 primary


Once a three node Mongo replica set is configured on the Kubernetes nodes
1. Kill the node running the primary
2. Mongo schedules a leader election electing one of the secondaries as a new primary
3. Kubernetes reschedules the old primary onto our 4th Kubernetes minion node.
4. the newly reestablished replica catches up to the 3 nodes
5. election happens again if needed.


Mongo database will have to be seeded with an arbitrary 1gb of seeded data to better simulate a production database going through the rescheduling, re-election, and data catch up process.


a few issues needing to work out
1. how to route writes to newly elected leader with Kubernetes
2. how to route reads to all replica nodes (if you want to)
3. how to constrain pods generated by 3 separate replication controllers to separate nodes (resource constraints)
4. how to configure each new node on first deployment to be setup correctly in the replica set.

How to handle automatic ``rs.initiate(config)` setup on `kubctl create -f rc.yml'